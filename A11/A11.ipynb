{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Logistic Regression and kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "labels_and_feats  = pandas.read_csv(\"data.csv\", header=None, usecols=range(1, 32)).to_numpy()\n",
    "np.random.shuffle(labels_and_feats)\n",
    "\n",
    "label_map = {\n",
    "    'B': 0,\n",
    "    'M': 1,\n",
    "}\n",
    "labels = np.vectorize(lambda x: np.float32(label_map[x]))(labels_and_feats[:, 0])\n",
    "\n",
    "feats = np.vectorize(lambda x: np.float32(x))(labels_and_feats[:, 1:31])\n",
    "\n",
    "for i in range(feats.shape[1]):\n",
    "    col = feats[:, i]\n",
    "    feats[:, i] = (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "num_recs = feats.shape[0]\n",
    "num_train = int(0.8 * num_recs)\n",
    "num_test = num_recs - num_train\n",
    "\n",
    "X_train = feats[0:num_train]\n",
    "y_train = labels[0:num_train]\n",
    "\n",
    "X_test = feats[num_train:][:num_test]\n",
    "y_test = labels[num_train:][:num_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20: Cost: 147.94, Acc: 0.88\n",
      "Epoch 02/20: Cost: 123.72, Acc: 0.89\n",
      "Epoch 03/20: Cost: 109.40, Acc: 0.91\n",
      "Epoch 04/20: Cost: 99.63, Acc: 0.92\n",
      "Epoch 05/20: Cost: 92.49, Acc: 0.92\n",
      "Epoch 06/20: Cost: 87.00, Acc: 0.93\n",
      "Epoch 07/20: Cost: 82.61, Acc: 0.93\n",
      "Epoch 08/20: Cost: 78.99, Acc: 0.93\n",
      "Epoch 09/20: Cost: 75.95, Acc: 0.93\n",
      "Epoch 10/20: Cost: 73.34, Acc: 0.94\n",
      "Epoch 11/20: Cost: 71.08, Acc: 0.94\n",
      "Epoch 12/20: Cost: 69.08, Acc: 0.95\n",
      "Epoch 13/20: Cost: 67.31, Acc: 0.95\n",
      "Epoch 14/20: Cost: 65.73, Acc: 0.95\n",
      "Epoch 15/20: Cost: 64.29, Acc: 0.95\n",
      "Epoch 16/20: Cost: 62.99, Acc: 0.95\n",
      "Epoch 17/20: Cost: 61.81, Acc: 0.95\n",
      "Epoch 18/20: Cost: 60.71, Acc: 0.95\n",
      "Epoch 19/20: Cost: 59.71, Acc: 0.95\n",
      "Epoch 20/20: Cost: 58.77, Acc: 0.95\n",
      "Test Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def initialize_weights(num_feats: float) -> np.ndarray:\n",
    "    return np.random.random(num_feats) - 0.5\n",
    "\n",
    "def compute_cost(Xs: np.ndarray, ys: np.ndarray, W: np.ndarray, b: float) -> float:\n",
    "    y_preds = predict_logistic_regression(Xs, W, b)\n",
    "    return np.sum(-ys * np.log(y_preds) - (1 - ys) * np.log(1 - y_preds)), (np.count_nonzero(np.abs(ys - y_preds) < 0.5) / Xs.shape[0])\n",
    "\n",
    "def optimize_weights(Xs: np.ndarray, ys: np.ndarray, W: np.ndarray, b: float, lr: float) -> None:\n",
    "    y_preds = predict_logistic_regression(Xs, W, b)\n",
    "    dy_preds = (1 - ys) / (1 - y_preds) - ys / y_preds\n",
    "\n",
    "    z = np.dot(W, Xs.T) + b\n",
    "    dzs = sigmoid(z) * (1 - sigmoid(z)) * dy_preds\n",
    "\n",
    "    W -= lr * np.dot(dzs, Xs)\n",
    "    b -= lr * np.sum(dzs)\n",
    "\n",
    "def train_logistic_regression(Xs: np.ndarray, ys: np.ndarray, lr: float, num_epochs: int) -> None:\n",
    "    W = initialize_weights(Xs.shape[1])\n",
    "    b = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        optimize_weights(Xs, ys, W, b, lr)\n",
    "        cost, acc = compute_cost(Xs, ys, W, b)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{num_epochs}: Cost: {cost:.2f}, Acc: {acc:.2f}\")\n",
    "\n",
    "    return W, b\n",
    "\n",
    "def predict_logistic_regression(Xs: np.ndarray, W: np.ndarray, b: float) -> np.ndarray:\n",
    "    z = np.dot(W, Xs.T) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "W, b = train_logistic_regression(X_train, y_train, 0.001, 20)\n",
    "\n",
    "test_acc = np.count_nonzero(np.abs(y_test - predict_logistic_regression(X_test, W, b)) < 0.5) / y_test.shape[0]\n",
    "print(f\"Test Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1: Test Acc: 0.96\n",
      "k=3: Test Acc: 0.97\n",
      "k=5: Test Acc: 0.99\n",
      "k=7: Test Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(X1: np.ndarray, X2: np.ndarray) -> float:\n",
    "    return np.sum(np.square(X1 - X2)) ** 0.5\n",
    "\n",
    "def get_neighbors(X_train: np.ndarray, X_test_instance: np.ndarray, k: int) -> np.ndarray:\n",
    "    return np.argsort(np.apply_along_axis(lambda X: euclidean_distance(X_test_instance, X), 1, X_train))[:k]\n",
    "\n",
    "def predict_kNN(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, k: int) -> np.ndarray:\n",
    "    labels_from_idxs = np.vectorize(lambda idx: y_train[idx])\n",
    "    max_count_label_from_unique_counts = lambda unique_counts: unique_counts.values[np.argsort(unique_counts.counts)[-1]]\n",
    "    return np.apply_along_axis(lambda X: max_count_label_from_unique_counts(np.unique_counts(labels_from_idxs(get_neighbors(X_train, X, k)))), 1, X_test)\n",
    "\n",
    "for k in [1, 3, 5, 7]:\n",
    "    test_acc = np.count_nonzero((predict_kNN(X_train, y_train, X_test, k) - y_test) == 0) / y_test.shape[0]\n",
    "    print(f\"k={k}: Test Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Analysis\n",
    "\n",
    "- Both models perform very well, perhaps because the dataset is linearly separable (the logit of the logistic regression model, and the euclidean distance of the kNN model would benefit from this).\n",
    "- For the kNN model, we can observe that even small values of k (3 and 5), perform well, however, the larger values (5 and 7) perform even better.\n",
    "- Logistic Regression\n",
    "    - Strengths\n",
    "        - Computationally cheap\n",
    "        - Model weights provide interpretability\n",
    "    - Weaknesses\n",
    "        - May diverge during training\n",
    "        - Poor accuracy on multi-class prediction tasks\n",
    "- kNN\n",
    "    - Strengths\n",
    "        - Matter of convergence/divergence is not an issue\n",
    "        - Decent accuracy on multi-class prediction tasks\n",
    "    - Weaknesses\n",
    "        - Computationally expensive\n",
    "        - Difficult to interpret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
